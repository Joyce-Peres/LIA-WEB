# ğŸ¤Ÿ Scripts de Machine Learning - LIA-WEB

Este diretÃ³rio contÃ©m os scripts Python para treinar e usar o modelo de reconhecimento de gestos em Libras.

## ğŸ“‹ PrÃ©-requisitos

```bash
# Criar ambiente virtual (recomendado)
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac

# Instalar dependÃªncias
pip install -r requirements.txt
```

## ğŸ”„ Fluxo Completo de Treinamento

### 1ï¸âƒ£ Coletar Dados

```bash
python coletar_gestos.py
```

- Abre a webcam para capturar gestos
- Use **ESPAÃ‡O** para iniciar/parar gravaÃ§Ã£o
- Use **ESC** para cancelar
- Colete pelo menos **15 amostras** de cada gesto

**Controles:**
- Digite o nome do gesto (ex: A, B, OI, TCHAU)
- ESPAÃ‡O: Iniciar/Salvar gravaÃ§Ã£o
- ESC: Cancelar gravaÃ§Ã£o atual
- Digite 'sair' para encerrar

### 2ï¸âƒ£ Treinar Modelo

```bash
python treinar_modelo.py
```

OpÃ§Ãµes:
```bash
python treinar_modelo.py --epochs 50        # Mais Ã©pocas
python treinar_modelo.py --min-amostras 20  # Exigir mais amostras
python treinar_modelo.py --test-size 0.3    # 30% para teste
```

**SaÃ­da:**
- `modelos/modelo_gestos.h5` - Modelo Keras
- `modelos/rotulador_gestos.pkl` - Encoder de classes

### 3ï¸âƒ£ Testar Reconhecimento

```bash
python reconhecer_gestos.py
```

OpÃ§Ãµes:
```bash
python reconhecer_gestos.py --confianca 0.8  # Aumentar threshold
python reconhecer_gestos.py --alvo A         # Validar gesto especÃ­fico
```

**Controles:**
- ESC: Sair
- R: Resetar buffer
- V: Alternar modo verbose
- T: Definir gesto alvo

### 4ï¸âƒ£ Converter para Web

```bash
python converter_para_web.py
```

**SaÃ­da:**
- `src/assets/models/model.json` - Modelo TensorFlow.js
- `src/assets/models/metadata.json` - Metadata atualizado
- `src/app/core/data/gesture-labels.ts` - Labels TypeScript

## ğŸ“Š Estrutura de Dados

### Formato de entrada do modelo

```
Input shape: (batch, 30, 126)
              â”‚     â”‚    â””â”€â”€ 126 features = 21 landmarks Ã— 3 coords Ã— 2 mÃ£os
              â”‚     â””â”€â”€ 30 timesteps (frames)
              â””â”€â”€ batch size
```

### Landmarks MediaPipe

Cada mÃ£o tem 21 landmarks, cada um com coordenadas (x, y, z):
- 0: Pulso
- 1-4: Polegar
- 5-8: Indicador
- 9-12: MÃ©dio
- 13-16: Anelar
- 17-20: Mindinho

## ğŸ¯ Gestos Suportados (exemplo)

O modelo pode ser treinado para reconhecer qualquer gesto. Lista de gestos tÃ­picos:

**Alfabeto:** A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z

**NÃºmeros:** 1, 2, 3, 4, 5, 6, 7, 8, 9, 10

**SaudaÃ§Ãµes:** OI, TCHAU, OBRIGADO, DESCULPA, POR FAVOR, TUDO BEM

**Dias:** SEGUNDA-FEIRA, TERÃ‡A-FEIRA, QUARTA-FEIRA, QUINTA-FEIRA, SEXTA-FEIRA, SABADO, DOMINGO

## ğŸ› SoluÃ§Ã£o de Problemas

### CÃ¢mera nÃ£o abre
- Verifique se a webcam estÃ¡ conectada
- Feche outros aplicativos usando a cÃ¢mera
- Tente `cv2.VideoCapture(1)` se tiver mÃºltiplas cÃ¢meras

### Baixa acurÃ¡cia
- Colete mais amostras (mÃ­nimo 15-20 por gesto)
- Varie iluminaÃ§Ã£o e Ã¢ngulos durante coleta
- Aumente nÃºmero de Ã©pocas de treinamento
- Reduza nÃºmero de classes (gestos muito similares confundem)

### Erro de memÃ³ria
- Reduza batch_size no treinamento
- Feche outros programas pesados

## ğŸ“ Estrutura de DiretÃ³rios

```
lia-web/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ coletar_gestos.py
â”‚   â”œâ”€â”€ treinar_modelo.py
â”‚   â”œâ”€â”€ reconhecer_gestos.py
â”‚   â””â”€â”€ converter_para_web.py
â”œâ”€â”€ dados/
â”‚   â””â”€â”€ gestos_libras.csv
â”œâ”€â”€ modelos/
â”‚   â”œâ”€â”€ modelo_gestos.h5
â”‚   â””â”€â”€ rotulador_gestos.pkl
â””â”€â”€ src/assets/models/
    â”œâ”€â”€ model.json
    â”œâ”€â”€ group1-shard1of1.bin
    â””â”€â”€ metadata.json
```
